import sys
import io
sys.stdin = io.TextIOWrapper(sys.stdin.buffer, encoding='utf-8')

from time import time
import numpy as np
import re
import json
from hashlib import sha256
import asyncio
# modules
from config import *
from function.utile.data_analy import data_analysis, get_data
from function.utile.openai_util import OpenAIChat
from function.utile.chroma_util import ChromaDB


### Setting ###
# OPENAI_API_KEY = r""
# DATA_PATH = r""

class SmartAssistant:
    def __init__(self):
        # OpenAI API ì´ˆê¸°í™”
        self.openai_api = OpenAIChat(OPENAI_API_KEY)
        print("OpenAI API ì´ˆê¸°í™” ì™„ë£Œ!")
        # ChromaDB ì´ˆê¸°í™”
        self.chroma = ChromaDB()
        print("ChromaDB ì´ˆê¸°í™” ì™„ë£Œ!")
        
    def analysis(self):
        ### ë°ì´í„° ë¶„ì„ ###
        data_analysis(DATA_PATH)

    def data_add_chromadb(self):
        ### ë°ì´í„° ì •ì œ ë° ë¡œì»¬ ì €ì¥ ###
        data = get_data(DATA_PATH)

        # ë°ì´í„° ì €ì¥ Sequence
        temp_data = {}
        for i, (k, v) in enumerate(data.items(), start=1):
            # OpenAI Embedding íšë“ (title -> embedding)
            embedding = self.openai_api.get_embedding(k, OPENAI_EMBED_NAME)
            # keyë¥¼ í•´ì‹±í•˜ì—¬ id ìƒì„±
            id = sha256(k.encode("utf-8")).hexdigest()

            # temp_dataì— ì €ì¥
            temp_data[k] = {
                "id": id,
                "content": v,
                "embedding": embedding
            }
            print(f"{i}ë²ˆ ë°ì´í„° ì €ì¥ ì™„ë£Œ : {k}")
            # if i == 20:
            #     break

        with open("./smart/data/key_embed.json", "w", encoding="utf-8") as f:
            json.dump(temp_data, f, ensure_ascii=False, indent=2)

        ### Local ë°ì´í„° ChromaDBì— ì €ì¥ ###
        with open("./smart/data/key_embed.json", "r", encoding="utf-8") as f:
            temp_data = json.load(f)

        # ChromaDBì— ì €ì¥: title -(sha256)-> id
        for i, (k, v) in enumerate(temp_data.items(), start=1):
            v["id"] = sha256(k.encode("utf-8")).hexdigest()
            
            self.chroma.add_data(id=str(v["id"]),
                            title=k, 
                            content=v["content"],
                            embedding=v["embedding"])
            print(f"{i}ë²ˆ ë°ì´í„° ChromaDB ì €ì¥ ì™„ë£Œ : {k}")
            
        with open("./smart/data/temp_data.json", "w", encoding="utf-8") as f:
            json.dump(temp_data, f, ensure_ascii=False, indent=2)

    def search_db(self, query_text):
        ### ë°ì´í„° ê²€ìƒ‰ ###
        # OpenAI Embedding íšë“ (query_text -> embedding)
        query_embedding = self.openai_api.get_embedding(query_text, OPENAI_EMBED_NAME)
        # ChromaDBì— ì €ì¥ëœ ë°ì´í„°ì™€ ìœ ì‚¬ë„ ê²€ìƒ‰
        result = self.chroma.collection.query(
            query_embeddings=[query_embedding],
            n_results=20,
            include=["documents", "metadatas", "distances"]
        )
        # ê²°ê³¼ ì¶œë ¥
        output = []
        for i in range(len(result["documents"][0])):
            output.append({
                "title": result["documents"][0][i],
                "content": result["metadatas"][0][i]["content"],
                "distance": result["distances"][0][i]
            })

        for i, v in enumerate(output):
            print(f"{i+1}ë²ˆ ê²°ê³¼ : {v['title']}")
            print(f"ê±°ë¦¬ : {v['distance']}")
            print(f"ë‚´ìš© : {v['content']}")
            print("-"*50)

        return output

    async def chat(self):
        
        conversation_list = []
        system_prompt = """# Identity

ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê³ ê°ì§€ì› ìƒë‹´ ì±—ë´‡ì…ë‹ˆë‹¤.

# Instructions

* ìƒë‹´ì‚¬ëŠ” userì˜ ì§ˆë¬¸ì— ëŒ€í•´ ì¹œì ˆí•˜ê³  ì •í™•í•˜ê²Œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤.
* ëª¨ë“  ë©”ì‹œì§€ë¥¼ ì§€ì†ì ì¸ ëŒ€í™”ì˜ ì¼ë¶€ë¡œ ê°„ì£¼í•˜ì„¸ìš”.
* userê°€ ì§ˆë¬¸í•œ ë‚´ìš©ì— ì´ˆì ì„ ë§ì¶° ë‹µë³€ì„ ìœ ì§€í•˜ì„¸ìš”.
* í™˜ê°ì´ë‚˜ ê¾¸ë©°ë‚¸ ì •ë³´ëŠ” í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”.

# Example Behavior

If the user says: "ë°°ì†¡ë¹„ê°€ ì–¼ë§ˆì¸ê°€ìš”?"  
â†’ You might respond: "ê¸°ë³¸ ë°°ì†¡ë¹„ëŠ” 3,000ì›ì…ë‹ˆë‹¤. ë‹¨, ì œì£¼ ë° ë„ì„œì‚°ê°„ ì§€ì—­ì€ ì¶”ê°€ ìš”ê¸ˆì´ ë¶€ê³¼ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."  
""".strip()

        # ì‚¬ìš©ì ì…ë ¥ ë£¨í”„
        print("ìƒë‹´ ì‹œì‘! 'exit' ì…ë ¥ ì‹œ ì¢…ë£Œë©ë‹ˆë‹¤.\n")
        while True:
            # ì‚¬ìš©ì ì§ˆë¬¸ ì…ë ¥
            try:
                print("ì‚¬ìš©ì: ", end='', flush=True)
                raw_input = sys.stdin.buffer.readline()
                user_input = self.openai_api.embed_clean_text(raw_input)
                start_time = time()
            except Exception as e:
                print(f"ì…ë ¥ ì˜¤ë¥˜: {e}")
                continue
            # ì¢…ë£Œ ì¡°ê±´
            if user_input.lower().strip() == 'exit':
                print("ìƒë‹´ ì¢…ë£Œ.")
                break

            # ê´€ë ¨ chunk ê²€ìƒ‰
            ###
            # rag_results = self.search_db(user_input)
            conditional_query, rag_results = await self.ai_db_search(user_input, conversation_list)
            ###
            print(f"ê²€ìƒ‰ í¬í•¨ í‚¤ì›Œë“œ: {conditional_query[0]}")
            print(f"ê²€ìƒ‰ ì œì™¸ í‚¤ì›Œë“œ: {conditional_query[1]}")

            if len(rag_results) == 0:
                print("ì €ëŠ” ìŠ¤ë§ˆíŠ¸ ìŠ¤í† ì–´ FAQë¥¼ ìœ„í•œ ì±—ë´‡ì…ë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ ìŠ¤í† ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
                continue
            else:
                context = "\n\n".join(
                    [f"[{i+1}ë²ˆ] Question: {r['title']}:\nAnswer: {r['content']}" for i, r in enumerate(rag_results)]
                )
            
            rag_prompt = f"""# Instructions

ë‹¤ìŒì€ ë‹¤ë¥¸ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ê³ ê°ë“¤ì´ ë‚¨ê¸´ Q&A Records ì…ë‹ˆë‹¤.  
userì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ëŒ€í•´ Q&A Recordsì˜ ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.

# Context: Q&A Records

{context}

# Answer Guidelines

* userì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì— ëŒ€í•´ Q&A Recordsì˜ ë‚´ìš©ë§Œ ì°¸ê³ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
* í•­ëª©ì´ ì—¬ëŸ¬ ê°œì¸ ê²½ìš° ë²ˆí˜¸ë¥¼ ë¶™ì—¬ ë‚˜ì—´í•˜ì„¸ìš”.
* í•„ìš” ì‹œ êµ¬ë¶„ëœ ì†Œì œëª©(ì˜ˆ: ë°°ì†¡ ë°©ë²•, ì‹ ì²­ ì„œë¥˜)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ë„ ì¢‹ìŠµë‹ˆë‹¤.
* "Q&A Recordsì˜ ë‚´ìš©"ê³¼ "userì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸"ì´ ì„œë¡œ ê´€ë ¨ ì—†ì–´ì„œì„œ ë‹µë³€ì´ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°, 
  "<system_message> ê´€ë ¨ ë‚´ìš©ì´ ì—†ì–´ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."ë¼ê³ ë§Œ ë‹µë³€í•˜ì„¸ìš”.
* Q&A Recordsì— ì—†ëŠ” ì •ë³´ë¥¼ ë§Œë“¤ì–´ë‚´ê±°ë‚˜ ê°€ì •í•˜ì§€ ë§ˆì„¸ìš”.

# Output Format

* ë‹µë³€ì— ì°¸ê³ í•œ Q&A Recordsê°€ ìˆë‹¤ë©´, ê·¸ ë²ˆí˜¸ë¥¼ ë‹µë³€ ë§ˆì§€ë§‰ì— ë°˜ë“œì‹œ ê¸°ì…í•˜ì„¸ìš”. 
  Format: "<system_message> [1, 3, 5]'

# Examples

<user_query>
ì‚¬ì¥ë‹˜ ë³´í—˜ì— ëŒ€í•´ ì•Œë ¤ì¤˜ì¤˜
</user_query>

<assistant_response>
ì‚¬ì¥ë‹˜ ë³´í—˜ì— ëŒ€í•œ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

1. **ë³´í—˜ ì¢…ë¥˜**: ì‚¬ì¥ë‹˜ ë³´í—˜ì€ í¬ê²Œ '4ëŒ€ë³´í—˜/ì˜ë¬´ë³´í—˜/ì¼ë°˜ë³´í—˜' ì„¸ ê°€ì§€ ìœ í˜•ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
   - 4ëŒ€ë³´í—˜/ì˜ë¬´ë³´í—˜: ë²•ì ìœ¼ë¡œ ì‚¬ì—…ì˜ ê·œëª¨ì™€ ì—…ì¢…ì— ë”°ë¼ í•„ìˆ˜ ê°€ì…í•´ì•¼ í•˜ëŠ” ë³´í—˜ì…ë‹ˆë‹¤.
   - ì¼ë°˜ë³´í—˜: í•„ìš”í•  ê²½ìš° ì„ íƒì ìœ¼ë¡œ ê°€ì…í•  ìˆ˜ ìˆëŠ” ë³´í—˜ì…ë‹ˆë‹¤.

2. **ê°€ì… ì˜ë¬´**: 4ëŒ€ë³´í—˜ì€ ê·¼ë¡œìë¥¼ ì‚¬ìš©í•˜ëŠ” ì‚¬ì—…ì¥ì—ì„œëŠ” í•„ìˆ˜ë¡œ ê°€ì…í•´ì•¼ í•©ë‹ˆë‹¤. ë¯¸ê°€ì… ì‹œ ì‚¬ì—…ì£¼ì—ê²Œ ë²•ì  ì±…ì„ì´ ë”°ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

3. **ë¬¸ì˜ ë°©ë²•**: ë³´í—˜ ìƒí’ˆì˜ ë³´ì¥ ë‚´ìš© ë° ê°€ì… ê´€ë ¨ ë¬¸ì˜ëŠ” ì œíœ´ ë³´í—˜ì‚¬ ë˜ëŠ” í˜„ì¬ ì´ìš© ì¤‘ì¸ ë³´í—˜ì‚¬ë¥¼ í†µí•´ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œíœ´ ë³´í—˜ì‚¬ë¡œëŠ” í˜„ëŒ€í•´ìƒ, KBì†í•´ë³´í—˜, ì‚¼ì„±í™”ì¬, í¥êµ­í™”ì¬ ë“±ì´ ìˆìŠµë‹ˆë‹¤.

ìì„¸í•œ ì‚¬í•­ì€ ë³´í—˜ì‚¬ì— ì§ì ‘ ë¬¸ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
<system_message> [4, 2]
</assistant_response>

<user_query>
ë„¤ì´ë²„ì—ì„œ ì‚¬ì§„ ì°ê¸° ì¢‹ì€ ì¥ì†Œ ì•Œë ¤ì¤˜ì¤˜ 
</user_query>

<assistant_response>
<system_message> ê´€ë ¨ ë‚´ìš©ì´ ì—†ì–´ ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.
</assistant_response>
""".strip()
            # print(rag_prompt)
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ì™€ ì§€ë‚œ ëŒ€í™” ê¸°ë¡ ì¶”ê°€
            messages = []
            messages.append({"role": "system", "content": system_prompt})
            messages.extend(conversation_list[-20:])
            messages.append({"role": "system", "content": rag_prompt})
            messages.append({"role": "user", "content": user_input})
            
            # OpenAI API í˜¸ì¶œ
            assistant_reply = await self.openai_api.run_chat(
                                                messages=messages,
                                                model_name=OPENAI_MODEL_NAME
                                                )
            # print(f"AI ìƒë‹´ì‚¬: {assistant_reply}")

            # ë‹µë³€ íŒŒì‹±
            # Q&A ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš° ì²˜ë¦¬ë¦¬
            if "ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in assistant_reply:
                print("ì €ëŠ” ìŠ¤ë§ˆíŠ¸ ìŠ¤í† ì–´ FAQë¥¼ ìœ„í•œ ì±—ë´‡ì…ë‹ˆë‹¤. ìŠ¤ë§ˆíŠ¸ ìŠ¤í† ì–´ì— ëŒ€í•œ ì§ˆë¬¸ì„ ë¶€íƒë“œë¦½ë‹ˆë‹¤.")
                
            else: # ë‹µë³€ì— ì°¸ê³ í•œ Q&Aê°€ ìˆëŠ” ê²½ìš°
                match = re.search(r"\s*\[([\d,\s]+)\]", assistant_reply)
                if not match: # í•˜ì§€ë§Œ ë‹µë³€ì— ì í•©í•˜ì§€ ì•Šì€ ê²½ìš°
                    print("ë‹µë³€ì— ì°¸ê³ í•  Q&Aê°€ ì—†ìŠµë‹ˆë‹¤. ìƒë‹´ì‚¬ì—ê²Œ ì—°ê²°í•´ë“œë¦´ê¹Œìš”?")
                    
                else: # ì í•©í•œ ë‹µë³€ì„ ìƒì„±í•œ ê²½ìš° - ì—°ê´€ ì§ˆë¬¸ ì¶”ì²œ
                    # ì°¸ê³  Q&A ë²ˆí˜¸ ì¶”ì¶œ
                    refer_numbers = match.group(1)
                    refer_numbers = [int(num.strip()) for num in refer_numbers.split(",")]
                    # print(f"ì°¸ê³ í•œ ì§ˆë¬¸ ë²ˆí˜¸: {refer_numbers}")

                    # system_message ë¶€ë¶„ ì œê±°
                    cleaned_reply = re.sub(r"<system_message>\s*\[[\d,\s]+\]", "", assistant_reply).strip()
                    print("ì±—ë´‡:", cleaned_reply)

                    # ì°¸ê³  ì§ˆë¬¸ ëª©ë¡
                    print("-"*50)
                    print("<ë‹¤ìŒê³¼ ê°™ì€ Q&Aë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.>")
                    refer_list = []
                    for i in refer_numbers:
                        refer_list.append(rag_results[int(i)-1]["title"])
                        print(f"ì°¸ê³  ì§ˆë¬¸: {rag_results[int(i)-1]['title']}")
                    # ì°¸ê³ í•œ ì§ˆë¬¸ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰
                    refer_query = " ".join(refer_list)
                    conditional_query, recommend = await self.ai_db_search(refer_query, conversation_list)
                    # ì¶”ì²œ ì§ˆë¬¸ ëª©ë¡
                    print("<ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë„ ì¶”ì²œí•©ë‹ˆë‹¤.>")
                    recommend_list = []
                    for dict_item in recommend:
                        if len(recommend_list) >= 3:
                            break
                        if dict_item["title"] not in refer_list:
                            recommend_list.append(dict_item["title"])
                            print(f"ì¶”ì²œ ì§ˆë¬¸: {dict_item['title']}")
                    print("-"*50)

            # ëŒ€í™” ê¸°ë¡ ì—…ë°ì´íŠ¸
            conversation_list.append({"role": "user", "content": user_input})
            conversation_list.append({"role": "assistant", "content": assistant_reply})
            print(len(conversation_list))
            end_time = time()
            process_time = end_time - start_time
            print(f"ì²˜ë¦¬ ì‹œê°„: {process_time:.2f}ì´ˆ")

    async def ai_db_search(self, query_text, conversation_list):
        ### ë°ì´í„° ê²€ìƒ‰ ###
        # OpenAI ì§ˆë¬¸ ê·œê²©í™”
        system_prompt = """# Identity

ë‹¹ì‹ ì€ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ìƒë‹´ ì±—ë´‡ì—ì„œ ì‚¬ìš©í•  ì¿¼ë¦¬ íŒŒì„œë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ì—­í• ì€ ì‚¬ìš©ì ì§ˆë¬¸ì—ì„œ ê²€ìƒ‰ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œ(include)ì™€
ê²€ìƒ‰ì—ì„œ ì œì™¸í•´ì•¼ í•  í‚¤ì›Œë“œ(exclude)ë¥¼ ì¶”ì¶œí•˜ì—¬ êµ¬ì¡°í™”ëœ JSONìœ¼ë¡œ ë°˜í™˜í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

# Instructions

* ì´ì „ ëŒ€í™”ì˜ íë¦„ì„ ì°¸ê³ í•˜ë˜, userì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì„ ê¸°ì¤€ìœ¼ë¡œ "include"ì™€ "exclude"ë¥¼ íŒë‹¨í•˜ì„¸ìš”.
""".strip()

        parsing_prompt = """# Instructions

* ìœ„ ëŒ€í™” íë¦„ì„ ê³ ë ¤í•˜ì—¬, userì˜ ë§ˆì§€ë§‰ ì§ˆë¬¸ì„ ì •í™•íˆ ë¶„ì„í•´ì„œ ê²€ìƒ‰ìš© í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
* ë°˜ë“œì‹œ ì•„ë˜ì™€ ê°™ì€ JSON í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì•¼ í•©ë‹ˆë‹¤. ë‹¤ë¥¸ í…ìŠ¤íŠ¸ëŠ” ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”
  í˜•ì‹: {"include": [...], "exclude": [...]}
* "include"ì—ëŠ” ì‚¬ìš©ìê°€ ì•Œê³  ì‹¶ì€ í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì—¬ ì…ë ¥í•˜ì„¸ìš”. 
* "exclude"ì—ëŠ” 'ì œì™¸', 'ë¹¼ê³ ', 'ë§ê³ ' ë“±ì˜ ëŒ€ìƒì´ ë˜ëŠ” í‚¤ì›Œë“œë§Œ ë„£ìœ¼ì„¸ìš”.
  ì œì™¸ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ "exclude": [] ë¡œ ì •í™•íˆ ì…ë ¥í•˜ì„¸ìš”.
* ë‹µë³€ì€ JSON í•˜ë‚˜ë¡œë§Œ ì¶œë ¥í•˜ì„¸ìš”.

# Examples

<user_query>
ë¬´ë£Œ ë°°ì†¡ì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë°°ì†¡ ë°©ë²• ì•Œë ¤ì¤˜
</user_query>

<assistant_response>
{"include":["ë°°ì†¡ ë°©ë²•"], "exclude":["ë¬´ë£Œ ë°°ì†¡"]}
</assistant_response>

<user_query>
ì…ì  ì‹ ì²­ ì‹œ í•„ìš”í•œ ì„œë¥˜
</user_query>

<assistant_response>
{"include":["ì…ì  ì‹ ì²­", "ì„œë¥˜"], "exclude":[]}
</assistant_response>

# Answer Guidelines

* ì ˆëŒ€ë¡œ ë‹¨ì–´ë¥¼ í•¨ë¶€ë¡œ ë°”ê¾¸ì§€ ë§ˆì„¸ìš”.
""".strip()
        
        messages = []
        messages.append({"role": "system", "content": system_prompt})
        messages.extend(conversation_list[-5:])
        messages.append({"role": "system", "content": parsing_prompt})
        messages.append({"role": "user", "content": query_text})
        # OpenAI API í˜¸ì¶œ
        assistant_reply = await self.openai_api.run_chat(
                                            messages=messages,
                                            model_name=OPENAI_MODEL_NAME
                                            )
        # print(f"AI ì¿¼ë¦¬ íŒŒì„œ: {assistant_reply}")

        # OpenAI Embedding íšë“ (query_text -> embedding)
        # standardized_qurey = json.loads(assistant_reply)
        assistant_reply = self.fix_extra_closing_brace(assistant_reply)
        standardized_qurey = self.safe_parse_json(assistant_reply)

        _include = standardized_qurey["include"]
        include_query = " ".join(_include)
        _exclude = standardized_qurey["exclude"]
        exclude_query = " ".join(_exclude)

        # print(f"ê²€ìƒ‰ í¬í•¨ í‚¤ì›Œë“œ: {include_query}")
        # print(f"ê²€ìƒ‰ ì œì™¸ í‚¤ì›Œë“œ: {exclude_query}")
        
        # ChromaDBì— ì €ì¥ëœ ë°ì´í„°ì™€ ìœ ì‚¬ë„ ê²€ìƒ‰
        output = await self.conditional_search(include=include_query, 
                                               exclude=exclude_query)
        
        conditional_query = (include_query, exclude_query)
        return conditional_query, output

    async def conditional_search(self, include, exclude, alpha=0.8):
        v_pos = await self.openai_api.get_embedding(include, OPENAI_EMBED_NAME)
        v_pos = np.array(v_pos)
        # ì œì™€ ì¡°ê±´ì´ ì—†ë‹¤ë©´ 0ë²¡í„°
        if exclude.strip():
            v_neg = await self.openai_api.get_embedding(exclude, OPENAI_EMBED_NAME)
            v_neg = np.array(v_neg)
            q_vec = v_pos/np.linalg.norm(v_pos) - alpha * v_neg/np.linalg.norm(v_neg)
            q_vec = q_vec/np.linalg.norm(q_vec)
        else:
            q_vec = v_pos/np.linalg.norm(v_pos)

        result = self.chroma.collection.query(
            query_embeddings=[q_vec],
            n_results=20,
            include=["documents", "metadatas", "distances"]
        )
        # ê²°ê³¼ ì¶œë ¥
        output = []
        for i in range(len(result["documents"][0])):
            if result["distances"][0][i] < 10.3:
                output.append({
                    "title": result["documents"][0][i],
                    "content": result["metadatas"][0][i]["content"],
                    "distance": result["distances"][0][i]
                })

        # for i, v in enumerate(output):
        #     print(f"{i+1}ë²ˆ ê²°ê³¼ : {v['title']}")
        #     print(f"ê±°ë¦¬ : {v['distance']}")
        #     # print(f"ë‚´ìš© : {v['content']}")
        #     print("-"*50)

        return output
    
    def safe_parse_json(self, text):
        """
        ë¬¸ìì—´ ë‚´ì—ì„œ JSON ê°ì²´ë¥¼ ì¶”ì¶œí•˜ê³ , íŒŒì‹± ê°€ëŠ¥í•œ ê°€ì¥ ì²« JSON ë¸”ë¡ë§Œ ë°˜í™˜.
        ì¤‘ë³µ ê´„í˜¸ë‚˜ ê¸°íƒ€ ì˜¤ë¥˜ê°€ ìˆì„ ê²½ìš° ì •ê·œì‹ìœ¼ë¡œ ë³´ì • ì‹œë„.
        """
        try:
            # 1ì°¨ ì •ìƒ íŒŒì‹± ì‹œë„
            return json.loads(text)
        except json.JSONDecodeError:
            # ì¤‘ë³µ ê´„í˜¸ ì œê±° ë˜ëŠ” ë¬¸ìì—´ ë‚´ JSON ë¸”ë¡ë§Œ ì¶”ì¶œ
            json_like = re.search(r'\{.*\}', text, re.DOTALL)
            if json_like:
                json_str = json_like.group()
                # âœ… "include": [...] ì™€ "exclude": [...] ì¶”ì¶œ
                inc_match = re.search(r'"include"\s*:\s*\[[^\]]*\]', json_str)
                exc_match = re.search(r'"exclude"\s*:\s*\[[^\]]*\]', json_str)
                if inc_match and exc_match:
                    recovered = '{' + inc_match.group() + ', ' + exc_match.group() + '}'
                    try:
                        return json.loads(recovered)
                    except json.JSONDecodeError as e3:
                        print("ğŸ”´ ë³µêµ¬ëœ ë¬¸ìì—´ íŒŒì‹± ì‹¤íŒ¨:", e3)
                    
            raise ValueError("ìœ íš¨í•œ JSON í˜•ì‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    def fix_extra_closing_brace(self, text: str) -> str:
        open_braces = text.count('{')
        close_braces = text.count('}')
        # ë‹«ëŠ” ì¤‘ê´„í˜¸ê°€ ë” ë§ë‹¤ë©´ ì´ˆê³¼ë§Œí¼ ì œê±°
        if close_braces > open_braces:
            diff = close_braces - open_braces
            for _ in range(diff):
                # ë’¤ì—ì„œë¶€í„° í•˜ë‚˜ì”© ì œê±°
                text = text[::-1].replace('}', '', 1)[::-1]
        return text

if __name__ == "__main__":
    smart = SmartAssistant()
    asyncio.run(smart.chat())

    # query_text = "ì›ì í…œê³¼ ì›ì ë”œì€ ì–´ë–¤ ì°¨ì´ì•¼?"
#     query_text = """ [ì›ì í…œ] ì›ì í…œê³¼ ì›ì ë”œ ë™ì¼ ê¸°ê°„ ì¤‘ë³µ ì œì•ˆ ì‹œ ì›ì í…œ ê°€ê²©ì€ ë³„ë„ë¡œ ìˆ˜ì •í•˜ì§€ ì•Šì•„ë„ ë˜ë‚˜ìš”?
# [ì›ì ë”œ] ì›ì ë”œì¸ê°€ìš”, ì›ë¿”ë”œ ì¸ê°€ìš”?
# [ì›ì ë”œ] ì›ì ë”œì€ ì–´ë–¤ ì„œë¹„ìŠ¤ì¸ê°€ìš”?
# [ì›ì ë”œ] ì›ì ë”œì—ì„œ íŒë§¤í•˜ê³  ì‹¶ìœ¼ë©´ ì–´ë–»ê²Œ í•˜ë©´ ë˜ë‚˜ìš”?
# [ê³µí†µ] 'ì›ì ë”œ/ì›ì í…œ' ìµœëŒ€í• ì¸ê°€ì™€ ì‹¤êµ¬ë§¤ê°€ê°€ ì™œ ë‹¤ë¥´ê²Œ ë³´ì´ë‚˜ìš”?
# """
    query_text = "ì•Œë¦¬ì— ë¬¼ê±´ì„ íŒ”ê³  ì‹¶ì€ë° ì–´ë–»ê²Œ í•´?" # 1.3 ì´í•˜
    # query_text = ""
    # asyncio.run(smart.db_test(query_text))